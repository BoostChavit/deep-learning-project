{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras, tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, LSTM\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(img, smaller):\n",
    "        \n",
    "    # Convert the image to gray scale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    ret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    if(smaller):\n",
    "        rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "\n",
    "        # Applying dilation on the threshold image\n",
    "        dilation = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, rect_kernel, iterations = 1)\n",
    "        # dilation = cv2.dilate(thresh1, rect_kernel, iterations = 1)\n",
    "    else:\n",
    "        rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (18, 18))\n",
    "\n",
    "        # Applying dilation on the threshold image\n",
    "        dilation = cv2.dilate(thresh1, rect_kernel, iterations = 2)\n",
    "\n",
    "    # Finding contours\n",
    "    contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL, \n",
    "                                                    cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingbox(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    contours = extract(img, False)\n",
    "\n",
    "    sum = 0\n",
    "    y_pred = []\n",
    "    boxes = []\n",
    "    im = img.copy()\n",
    "    crops = []\n",
    "\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        \n",
    "        # rect = cv2.rectangle(im2, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            \n",
    "        # Cropping the text block\n",
    "        cropped = img[y:y + h, x:x + w]\n",
    "\n",
    "        # extract each alphabet from each block\n",
    "        contours2 = extract(cropped, True)\n",
    "        \n",
    "        for i in contours2:\n",
    "            # count alphabets \n",
    "            sum += 1\n",
    "\n",
    "            x2, y2, w2, h2 = cv2.boundingRect(i)\n",
    "            \n",
    "            cropped2 = img[y+y2: y+y2+h2, x+x2: x+x2+w2]\n",
    "            \n",
    "            center_x = (x + x2 + w2 / 2) / img.shape[1]\n",
    "            center_y = (y + y2 + h2 / 2) / img.shape[0]\n",
    "            normalized_width = w2 / img.shape[1]\n",
    "            normalized_height = h2 / img.shape[0]\n",
    "\n",
    "            # Append the YOLO format coordinates to the boxes list\n",
    "            boxes.append([center_x, center_y, normalized_width, normalized_height])\n",
    "\n",
    "\n",
    "            # boxes.append([x+x2, y+y2, x+x2 + w2, y+y2 + h2]) \n",
    "\n",
    "            cropped2 = cv2.resize(cropped2, (64,64))\n",
    "            \n",
    "            crops.append(cropped2)\n",
    "\n",
    "            cropped2 = cropped2.reshape((1, 64, 64, 3))\n",
    "\n",
    "    \n",
    "    \n",
    "    order = np.argsort([x[0] for x in boxes])\n",
    "    boxes = [boxes[x] for x in order]\n",
    "    crops = [crops[x] for x in order]\n",
    "    \n",
    "    return sum, boxes, im, crops\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each classes(62 classes) has 55 images \n",
    "model_training_csv = os.path.join(os.getcwd(), 'alphabets/english.csv')\n",
    "\n",
    "# make dataframe for training model \n",
    "model_training_df = pd.read_csv(model_training_csv)\n",
    "\n",
    "class_names = model_training_df.label.unique()\n",
    "\n",
    "Y = [label for label in range(len(class_names))]\n",
    "\n",
    "# one-hot-encoded the label\n",
    "y_train = keras.utils.to_categorical(class_names, len(class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏û‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ\n",
    "source_folder_images = os.path.join(os.getcwd(), \"archive/train_v2/train\")\n",
    "\n",
    "# ‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏û‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô dataset YOLO\n",
    "destination_folder_images = os.path.join(os.getcwd(), \"data/images\")\n",
    "\n",
    "\n",
    "destination_folder_labels = os.path.join(os.getcwd(), \"data/labels\")\n",
    "\n",
    "# destination_folder_labels = os.path.join(os.getcwd(), \"dataset_yolo\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "24\n",
      "22\n",
      "10\n",
      "18\n",
      "23\n",
      "21\n",
      "18\n",
      "21\n",
      "24\n",
      "30\n",
      "22\n",
      "18\n",
      "29\n",
      "17\n",
      "18\n",
      "14\n",
      "30\n",
      "33\n",
      "19\n",
      "10\n",
      "15\n",
      "15\n",
      "14\n",
      "30\n",
      "33\n",
      "11\n",
      "27\n",
      "30\n",
      "16\n",
      "14\n",
      "27\n",
      "18\n",
      "14\n",
      "22\n",
      "10\n",
      "30\n",
      "25\n",
      "10\n",
      "28\n",
      "16\n",
      "30\n",
      "18\n",
      "21\n",
      "21\n",
      "24\n",
      "29\n",
      "16\n",
      "24\n",
      "16\n",
      "30\n",
      "14\n",
      "29\n",
      "13\n",
      "30\n",
      "25\n",
      "27\n",
      "10\n",
      "29\n",
      "21\n",
      "10\n",
      "11\n",
      "10\n",
      "27\n",
      "17\n",
      "19\n",
      "30\n",
      "21\n",
      "18\n",
      "14\n",
      "10\n",
      "28\n",
      "17\n",
      "21\n",
      "14\n",
      "34\n",
      "16\n",
      "10\n",
      "27\n",
      "24\n",
      "29\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "train_img_path = os.path.join(os.getcwd(), \"archive/train_v2/train\")\n",
    "test_img_path = os.path.join(os.getcwd(), \"archive/test_v2/test\")\n",
    "\n",
    "train_csv_path = os.path.join(os.getcwd(), \"archive/written_name_train_v2.csv\")\n",
    "test_csv_path = os.path.join(os.getcwd(), \"archive/written_name_test_v2.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(train_csv_path)\n",
    "cnt = 0\n",
    "im = 0\n",
    "boxes = []\n",
    "\n",
    "# for i in range (df.shape[0]):\n",
    "for i in range (100):\n",
    "    # print(df['FILENAME'][i])\n",
    "    img_name = os.path.join(train_img_path, df['FILENAME'][i])\n",
    "    num_of_char, boxes, img, crop = boundingbox(img_name)\n",
    "    # num_of_char, y_pred, boxes, img, crop = boundingbox(img_name, model)\n",
    "    \n",
    "    if num_of_char == len(str(df['IDENTITY'][i])):\n",
    "        cnt += 1\n",
    "        fileName = df['FILENAME'][i]\n",
    "        shutil.copy(os.path.join(source_folder_images, fileName), os.path.join(destination_folder_images, fileName))\n",
    "\n",
    "        label = ''\n",
    "        for j in range(len(df['IDENTITY'][i])):\n",
    "            if df['IDENTITY'][i][j] == ' ':\n",
    "                continue\n",
    "            name = np.where(class_names == df['IDENTITY'][i][j])[0][0]\n",
    "            print(name)\n",
    "            label += str(name) + ' '\n",
    "            temp = ' '.join(map(str, boxes[j]))\n",
    "            label += temp\n",
    "            label += '\\n'\n",
    "         \n",
    "\n",
    "        fileName = fileName.split('.')\n",
    "        new_filename = f\"{fileName[0]}.txt\"\n",
    "        label_path = os.path.join(destination_folder_labels, new_filename)\n",
    "\n",
    "        with open(label_path, \"w\") as label_file:\n",
    "            label_file.write(label)\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(class_names == '1')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.25 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.24 üöÄ Python-3.11.5 torch-2.2.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/Users/boost/Documents/deep_learning/project/data/coco128.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train10, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train10\n",
      "Overriding model.yaml nc=80 with nc=62\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2140042  ultralytics.nn.modules.head.Detect           [62, [128, 256, 512]]         \n",
      "Model summary: 225 layers, 11159594 parameters, 11159578 gradients, 28.8 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train10', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/boost/Documents/deep_learning/project/data/labels... 12 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 2935.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/boost/Documents/deep_learning/project/data/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/boost/Documents/deep_learning/project/data/labels.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train10/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000152, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train10\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      2.075      6.867      1.135        141        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         81    0.00034       0.05    0.00166    0.00133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G       2.16      7.113      1.073        176        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         81   0.000336       0.05    0.00184    0.00147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      2.199      7.618      1.063        137        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         81   0.000329       0.05    0.00216    0.00173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      2.202      6.705      1.109        205        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         81    0.00532     0.0583     0.0054    0.00345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      2.226      6.938      1.078        220        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         81    0.00531     0.0583    0.00508    0.00319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.012 hours.\n",
      "Optimizer stripped from runs/detect/train10/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/detect/train10/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/detect/train10/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.24 üöÄ Python-3.11.5 torch-2.2.1 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 11149578 parameters, 0 gradients, 28.6 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12         81    0.00532     0.0583     0.0054    0.00345\n",
      "                     A         12          9          0          0          0          0\n",
      "                     B         12          2          0          0          0          0\n",
      "                     D         12          1          0          0          0          0\n",
      "                     E         12          7          0          0          0          0\n",
      "                     F         12          2          0          0          0          0\n",
      "                     G         12          5          0          0          0          0\n",
      "                     H         12          3          0          0          0          0\n",
      "                     I         12          7          0          0          0          0\n",
      "                     J         12          2          0          0          0          0\n",
      "                     L         12          7          0          0          0          0\n",
      "                     M         12          3          0          0          0          0\n",
      "                     N         12          1          0          0          0          0\n",
      "                     O         12          5          0          0          0          0\n",
      "                     P         12          2          0          0          0          0\n",
      "                     R         12          6        0.1      0.167     0.0583     0.0292\n",
      "                     S         12          2          0          0          0          0\n",
      "                     T         12          5          0          0          0          0\n",
      "                     U         12          9          0          0          0          0\n",
      "                     X         12          2          0          0          0          0\n",
      "                     Y         12          1    0.00645          1     0.0498     0.0398\n",
      "Speed: 0.2ms preprocess, 37.0ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "yolo_folder = os.path.join(os.getcwd(), \"data\")\n",
    "model = YOLO(\"yolov8s.pt\")  # load the model\n",
    "\n",
    "# Specify the path to your training YAML file\n",
    "data_yaml = os.path.join(yolo_folder, \"coco128.yaml\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=data_yaml, epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
